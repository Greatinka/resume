Описание работы:

При работе с кредитными данными возникла проблема разнородных признаков: категориальные переменные 
(пол, жилье, цель кредита) и порядковые (размер сберегательного счета) требовали разной предобработки. 
Вместо того чтобы обрабатывать всё вручную каждый раз, были заранее подготовлены и сохранены энкодеры 
(joblib.dump) для OneHotEncoder и OrdinalEncoder, а также сохранен StandardScaler для единообразной 
нормализации новых данных. Это позволило в production-среде просто загружать готовые трансформеры и 
применять их к новым заявкам за миллисекунды.

При реализации проекта были перепробованы многие классификаторы: RandomForestClassifier, GradientBoostingClassifier 
с подбором гиперпараметров через GridSearchCV. Наилучший результат в опытах был достигнут при помощи градиентного 
бустинга - именно он показал стабильно высокие метрики на тестовой выборке.

Ключевым моментом стала правильная предобработка категориальных признаков. Для порядковой переменной "Saving accounts" 
использовался OrdinalEncoder с заданным порядком градаций (unknown → little → moderate → quite rich → rich). Для 
номинативных признаков был применен OneHotEncoder, чтобы не вносить ложных порядковых отношений.

Важно упомянуть, что при подборе гиперпараметров качество росло немонотонно - например, увеличение количества деревьев 
с 400 до 500 не всегда давало прирост, поэтому GridSearch с кросс-валидацией оказался критически важным для нахождения 
оптимальной комбинации n_estimators, max_depth и min_samples_leaf.

Перед обучением был проведен разведочный анализ: построены распределения возрастов и сумм кредита для понимания данных, 
проанализирована корреляционная матрица, чтобы исключить мультиколлинеарность. OneHotEncoder создал множество новых признаков, 
но корреляционный анализ показал, что они не дублируют друг друга.

Итоговый стек для выполнения проекта: Python, Pandas для обработки данных, Scikit-learn для препроцессинга, моделей и метрик, 
Joblib для сериализации моделей, Seaborn и Matplotlib для визуализации.
